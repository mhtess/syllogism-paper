\documentclass[floatsintext, man]{apa6}
\usepackage{apacite}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{centernot}
\usepackage{tabularx}
\usepackage{tikz}
\usetikzlibrary{bayesnet}


  \def\firstcircle{(90:1cm) circle (1.5cm)}
  \def\secondcircle{(210:1cm) circle (1.5cm)}
  \def\thirdcircle{(330:1cm) circle (1.5cm)}
  


% these packages are needed to insert results 
% obtained from R into the LaTeX document
\usepackage{pgfplotstable}
\usepackage{csvsimple}
\usepackage{siunitx}

% set the name of the folder in which the CSV files with 
% information from R is stored
\newcommand{\datafoldername}{./}

% the following code defines the convenience functions
% as described in the main text below

% rlgetvalue returns whatever is the in cell of the CSV file
% be it string or number; it does not format anything
\newcommand{\rlgetvalue}[4]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\myvalue}}

% rlgetvariable is a shortcut for a specific CSV file (myvars.csv) in which
% individual variables that do not belong to a larger chunk can be stored
\newcommand{\rlgetvariable}[1]{\csvreader[]{\datafoldername/myvars.csv}{#1=\myvar}{\myvar}\xspace}

% rlnum format a decimal number
\newcommand{\rlnum}[2]{\num[output-decimal-marker={.},
                             exponent-product = \cdot,
                             round-mode=places,
                             round-precision=#2,
                             group-digits=false]{#1}}

\newcommand{\rlnumsci}[2]{\num[output-decimal-marker={.},
                          scientific-notation = true,
                             exponent-product = \cdot,
                             round-mode=places,
                             round-precision=#2,
                             group-digits=false]{#1}}

\newcommand{\rlgetnum}[5]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\rlnum{\myvalue}{#5}}}

\newcommand{\rlgetnumsci}[5]{\csvreader[filter strcmp={\mykey}{#3},
             late after line = {{,}\ }, late after last line = {{}}]
            {\datafoldername/#1}{#2=\mykey,#4=\myvalue}{\rlnumsci{\myvalue}{#5}}}



\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother \def\signed
#1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip2em
\hbox{}\nobreak\hfil#1% \parfillskip=0pt \finalhyphendemerits=0
\endgraf}} \newsavebox\mybox 

\newenvironment{aquote}[1]
{\savebox\mybox{#1}\begin{quote}} {\signed{\usebox\mybox}\end{quote}}


\title{Logic, probability, and pragmatics in syllogistic reasoning}
\shorttitle{Pragmatics in Syllogistic Reasoning}

\author{Michael Henry Tessler\textsuperscript{1}\textsuperscript{,2}, Joshua B. Tenenbaum\textsuperscript{1},~and \\Noah D. Goodman\textsuperscript{2}\textsuperscript{,3}}
\date{}
  
\affiliation{
\vspace{0.5cm}
\textsuperscript{1} Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology \\
\textsuperscript{2} Department of Psychology, Stanford University \\
\textsuperscript{3} Department of Computer Science, Stanford University
}


\date{}

\usepackage{xcolor}
\usepackage{bbm}

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}

\authornote{Corresponding author: Michael Henry Tessler,  Department of Brain and Cognitive Sciences, Building 46, Room 3027,	Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139-4307, USA; tessler@mit.edu}

\keywords{syllogisms; reasoning; pragmatics; semantics; Rational Speech Act \newline\indent Word count: 7911}

\abstract{
Syllogistic reasoning lies at the intriguing intersection of natural and formal reasoning, of language and logic. 
Syllogisms comprise a formal system of reasoning yet make use of natural language quantifiers (e.g., \emph{all}, \emph{some}) and invite natural language conclusions. 
The conclusions people tend to draw from syllogisms, however, deviate substantially from the purely logical system. 
Are principles of natural language understanding to blame?
We introduce a probabilistic pragmatic perspective on syllogistic reasoning: We decompose reasoning with natural language arguments into two sub-problems, language comprehension and language production. 
We formalize models of these processes within the Rational Speech Act framework and explore the pressures that pragmatic reasoning places on the production of conclusions.
We test our models on a recent, large data set of syllogistic reasoning and find that the selection process of conclusions from syllogisms are best modeled as a pragmatic speaker who has the goal of aligning the beliefs of a naive listener with those of their own. 
We compare our model to previously published models that implement two alternative theories -- Mental Models and Probability Heuristics -- finding that our model quantitatively predicts the full distributions of responses as well as or better than previous accounts, but with far fewer parameters. 
Our results suggest that human syllogistic reasoning may be best understood not as a poor approximation to ideal logical reasoning, but rather as rational probabilistic inferences in support of natural communication.}

\begin{document}
\maketitle



\newpage

\section{Introduction}

Imagine discussing a recent tax proposal TR158 with a friend. Your friend remarks that in TR158:

\begin{quote}
Some of the taxes on the wealthy will be modified. \\
All of the tax modifications in the bill will expire in 10 years.
\end{quote}

From these remarks, it follows -- logically, necessarily -- that \emph{Some of the taxes on the wealthy will expire in 10 years}.
That is, in all situations in which the premises are true, the conclusion is also true. 
The argument is logically valid.
Now consider a superficially similar set of remarks: In TR158,

\begin{quote}
All of the taxes on the wealthy will be modified. \\
Some of the tax modifications in the bill will expire in 10 years.
\end{quote}

You may be tempted to draw a similar conclusion:  \emph{Some of the taxes on the wealthy will expire in 10 years}.
The argument seems reasonable, and yet the conclusion is not true in all situations in which the premises are true.
Consider, for instance, that there could be tax modifications applied to the non-wealthy, and that those modifications are the ones that will expire in 10 years.
Then, the premises would be true but the conclusion false: The argument is logically invalid.

Human intuitions about what conclusions reasonably follow from a set of premises seem to be supported by something other than pure logical necessity. 
Aristotle's invention of syllogisms -- the kinds of two premise arguments shown above -- can be interpreted, in fact, as an attempt to impose a simple rational structure on what might otherwise seem like fuzzy or messy human intuitions about logical relationships.
% Syllogisms are the foundation of modern logic.
In the modern era, cognitive psychologists have turned the syllogism back on the human mind, as a rational basis from which to evaluate human reasoning \cite{Storring1908}.
Perhaps unsurprisingly, how everyday human beings reason with syllogisms differs from what classical rules of logic would dictate \cite<for a meta-analysis, see:>{Khemlani2012}.



A syllogism is a formal logical argument that uses quantifiers (e.g. \emph{all}, \emph{some}) to relate two properties (e.g., \emph{taxes on the wealthy} and \emph{expiring in 10 years}) via a third (``middle'') property (e.g., \emph{being modified by the proposal}). 
Many theoretical frameworks have been brought to explain how humans reason with syllogisms.
Logic or syntactic-based approaches have attempted to articulate modified logical rules -- so-called ``natural logic'' -- that humans deploy when evaluating syllogistic arguments \cite{braine1983logical, rips1994, geurts2003reasoning}.
Process accounts have been developed to describe how we manipulate  model- or diagram-based mental representations to arrive at a conclusion, the most prominent example being the Mental Models theory \cite{johnson1975models, johnson1983mental, johnson2015logic}.
Still other approaches attempt to revisit the rational or functional basis of human logical reasoning, viewing deductive reasoning as a special case of probabilistic inference \cite{oaksford2001probabilistic, oaksford2007bayesian, hahn2007rationality, tenenbaum2006theory}; this approach is most prominently   formalized in the Probability Heuristics Model of syllogistic reasoning \cite{Chater1999}.



Though the substantial prior work provides insight into how humans reason with syllogisms, a unified, explanatory theory of human syllogistic reasoning remains outstanding \cite{Khemlani2012}. 
The most prominent accounts of syllogistic reasoning -- Mental Models Theory \cite{johnson1975models, johnson2015logic} and the Probability Heuristics Model  \cite{Chater1999} -- assume a standard competence-level theory involving first-order logic and probabilities, but require the addition of either process-level idiosyncrasies or heuristics presumed to arise from resource constraints in order to explain human judgments. 
Furthermore, the fact that syllogisms involve natural language demands a theory that can be integrated with principles of natural language  -- semantics and pragmatics -- which are also likely central in human reasoning \cite{sperber1986relevance,mercier2017enigma}.
Natural language semantics plays a role in many theories of reasoning \cite{JL1978, Khemlani2012, geurts2003reasoning}, but how those semantics interface with pragmatic reasoning is either posited in an ad-hoc manner \cite{Chater1999} or has been limited to simple Gricean enrichment of the premises \cite{Roberts2001}.	



We take up the task of developing a competence-level theory that builds upon first-order logic and probabilities but construes syllogistic reasoning as part of communication. 
Our approach is at the level of a computational theory (in the sense of \citeNP{marr1982vision}) or rational analysis (in the sense of \citeNP{anderson1990adaptive}) that seeks to model the computations underlying human syllogistic reasoning without stipulating a precise implementation of such a reasoning process. 
In particular, we view syllogistic reasoning through the lens of the language understanding processes that guide everyday reasoning with language, a process which involves reasoning about how an interlocutor would interpret a speaker's statements \cite{Grice1975, Clark1996, Levinson2000}. 
Specifically, we propose that human reasoning with syllogisms is composed of two key sub-tasks: A reasoner first interprets the premises of the syllogism by behaving like a listener interpreting natural language expressions, and then the reasoner produces a conclusion by acting like a speaker deciding what to say.
We formalize this hypothesis in a probabilistic model of pragmatic reasoning in the Rational Speech Act framework \cite{Frank2012a, goodman2016pragmatic, scontras2018probabilistic}.
Our model is able to explain a number of qualitative phenomena in syllogistic reasoning, including articulating a rational mechanism by which a person could respond \emph{nothing follows}, an outstanding problem in theories of syllogistic reasoning \cite{riesterer2020modeling}.
We additionally show that pragmatics enters into the conclusion selection process by way of participants reflecting on how well their choice of conclusion would convey their own beliefs to a naive listener. 
While there are almost certainly important processing details driven by resource limitations, we do not need to appeal to them in order to explain many aspects of human judgments.
And though we do not explain all of the variance in the human data, the model provides a path forward for addressing several outstanding issues including how pragmatics might enter into the interpretation of premises and  how the content of a syllogism can affect reasoning (known as \emph{belief bias}; \citeNP{Newstead1992, Oakhill1993, Klauer2000, Dube2011}).


\section{Rational Speech Act Models}

We develop a family of models of syllogistic reasoning as communication. 
A reasoner first behaves as a listener who updates their prior beliefs about the world given the premises of the syllogism; then, the reasoner acts as a speaker who is deciding what conclusion to say given their updated beliefs about the world (Figure \ref{fig:cartoon}). 
We describe our model in three stages: the ontology and semantics in the model, the listener model (or, how the premises get interpreted), and the speaker model (or, how the conclusion gets produced). 
In general, our approach is to have pragmatic inference (not merely literal interpretation) be a component of both our listener and speaker models \cite<cf.,>{Roberts2001}; in this paper, for simplicity, we only develop the pragmatic aspects of the probabilistic speaker model, restricting our probabilistic listener model to literal (though potentially noisy) interpretations.

\begin{figure}[t]
\centering
\includegraphics[width = \textwidth]{cartoon.pdf}
\caption{Overview of the probabilistic pragmatics perspective of syllogistic reasoning. The reasoner first acts as a listener (left), interpreting the premises of the syllogism by updating their prior beliefs about the state (Venn diagram) according to the truth-functional meanings of the quantifier relations in the premises, resulting in a posterior distribution over states. Then, the reasoner acts as a speaker (right), deciding among a set of conclusions which would be the best to produce. The model developed in this paper restricts the listener model to only literal (though potentially noisy) interpretations of the premises, while the speaker model entertains pragmatic considerations (i.e., by reasoning about a listener).}
\label{fig:cartoon}
\end{figure}


\subsection{Ontology and semantics}


The relations used in classical Aristotelean syllogisms are quantifiers: \emph{all}, \emph{some}, \emph{some .... are not}, and \emph{none}.
An example Aristotelean syllogism reads:

\begin{quote}
All artists are bakers. \\
Some bakers are chemists. \\
\_\_ \\
Some artists are chemists.
\end{quote}

The logical space defined by classical, Aristotelean syllogisms is comprised of all combinations of quantifiers and term orders for the premises (e.g., \textsc{quantifier} \textsc{A} -- \textsc{B} vs. \textsc{quantifier} \textsc{B} -- \textsc{A}).\footnote{In addition to being restricted to considering only the Aristotelean quantifiers, the classic syllogistic reasoning task further only makes use of the primitive terms \textsc{As}, \textsc{Bs}, and \textsc{Cs} (e.g., one may not come up with a conclusion referring to \textsc{non-As}).}
In total, each of the two premises can appear with one of the four quantifiers and two term orders yielding:  $4$ quantifiers  $\times$ 2 term orderings $\times$ 2 premises = 64 syllogistic premises.
Of the 64 unique syllogistic premises, only 27 yield a logically valid conclusion that uses the Aristotelean quantifiers (\emph{all}, \emph{some}, \emph{some...not}, \emph{none}): a conclusion that is true in every situation in which the premises are true.\footnote{Following \citeA{Khemlani2012} and others, we describe the syllogisms assuming ``existential import''; namely, that the premises describe categories that contain actual members (i.e., there are at least some artists, some bakers, and some chemists).}
For the remaining 37, several conclusions may follow from the premises, but there are no conclusions that are necessarily true given the premises; in terms of the syllogistic reasoning task, the ``correct response'' for these syllogistic premises is typically taken to be that ``nothing follows'' (or, ``No Valid Conclusion''; NVC). 



Our model is built on top of an ontology of states (or situations), which encode the types of objects that exist in a world. 
An object-type is represented by its properties.
Consider the second syllogism from above:

\begin{quote}
All artists are bakers. \\
Some bakers are chemists. \\
\_\_ \\
Some artists are chemists.
\end{quote}

One possible state that is consistent with this syllogism is:

\begin{tabularx}{.8\textwidth}{XXXXX}
& \\
\tt artist baker \\
\tt artist baker chemist \\
\tt \hspace{1.23cm} baker chemist 
& \\
& \\
\end{tabularx}

\noindent In this state, there are three unique types of objects or entities: an artist who is also a baker, a baker who is also a chemist, and artist who is also a baker and chemist.\footnote{
This formalism is different from a \emph{mental model} in the style of  \citeA{johnson1983mental}. In that formalism, one could have a model such as:

%\begin{figure}
\begin{tabularx}{.8\textwidth}{XXXXX}
%\begin{quotation} 
& \\
\tt artist baker \\
\tt artist baker chemist \\
\tt \hspace{1cm} baker chemist \\
\tt artist baker \\
& \\
%\end{quotation}
\end{tabularx}
%\caption{The 4 unique term-orderings of syllogisms}
%\label{figures}
%\end{figure}

\noindent where each row of the model denotes an individual entity that has the properties listed (e.g., the first row denotes an artist who is also a baker).
In this model, there are two artist-bakers, one baker-chemist, and one artist-baker-chemist.
The classical categorical syllogisms we model below involve the quantifiers \emph{all} and \emph{some} (and their logical negations); the truth conditions of these quantifiers (i.e., whether or not quantifiers are true) are not affected by the number of entities that have a particular combination of properties (i.e., it is inconsequential that there are two artist-bakers in the model above).
Hence, we can represent the model in a reduced format, in which only the unique object types are represented.
For generalized quantifiers (e.g., \emph{most}, \emph{few}), the number of objects that have unique combinations of properties will matter. We discuss how our approach can be generalized to handle reasoning with generalized quantifiers in the General Discussion.
}


\begin{figure}[h]
\centering
    \begin{tikzpicture}
      \begin{scope}
    \clip \firstcircle;
    \fill[gray] \secondcircle;
      \end{scope}
      \begin{scope}
    \clip \secondcircle;
    \fill[gray] \thirdcircle;
      \end{scope}
      \draw \firstcircle node[text=black,above] {$A$};
      \draw \secondcircle node [text=black,below left] {$B$};
      \draw \thirdcircle node [text=black,below right] {$C$};
    \end{tikzpicture}
\label{fig:venn}
\caption{A state is represented by Boolean values over the regions of a Venn diagram, corresponding to the presence or absence of unique object types. The diagram depicts a state in which there are ABs, BCs, ABCs, and no other object types.}
\end{figure}


This representation of states, in which only the unique object types are present, can also be displayed as a Venn diagram (Figure 1).
A region in the diagram corresponds to an object type (e.g., an A which is also a B and a C) and the Venn diagram contains eight regions $r \in R$, which correspond to all object types that could be referred to in the syllogism: $\{\{A,B,C\},$ $\{A,B,\neg C\},$ $\{A,\neg B,C\},$ $\{\neg A, B, C\},$ $\{A, \neg B, \neg C\},...\}$. For simplicity, when possible, we will refer to regions by the properties that are present, omitting the absent properties; for example, $\{A, B, \neg C\}$ will be referred to as $AB$.
Each region in the Venn diagram can take on a binary value, indicating the presence or absence of that unique object type.
Then, a state or situation (Venn diagram) $s \in S$ is composed of the set of object types (regions) that are present (e.g., $\{AB, BC, ABC\}$ is one possible state).
The set of unique states then has size $2^8 = 256$, however the empty region (or, the object type which has neither of the three properties: $\{\neg A, \neg B, \neg C\}$) does not impact the truth conditions of the quantifier sentences used in syllogisms, and so we consider only $2^7 = 128$ states.




The classical syllogisms are comprised by two premise arguments where each premise relates two properties via a quantifier. 
The quantifiers in classical syllogism are \emph{all}, \emph{some}, \emph{none}, and \emph{not all}.\footnote{
These quantifiers are typically presented in sentences such that \emph{none} is rendered as \emph{no} (e.g., No As are Bs) and \emph{not all} is rendered as \emph{some \_\_ are not} (e.g., Some As are not Bs).
}
We use the classic logical semantics of these quantifiers described in Table \ref{tab:sem}.
We assume the semantics of \emph{All As are Bs} describes a situation in which there exist objects that have the first property, sometimes referred to as the ``existential presupposition'' (e.g., \emph{All As are Bs} is false if there are no As, and hence \emph{All As are Bs} entails \emph{Some As are Bs}). 


\begin{table}[b]
\begin{tabular}{@{}llll@{}}
\toprule
Example syllogistic sentence & Logical Meaning                                                                       & Consistent State & Inconsistent state \\ \midrule
All As are Bs                                      & $\forall i \in R: A(i) \implies B(i) $ & \{AB, ABC\}                           & \{A, ABC\}                              \\
Some As are Bs                                     & $\exists i \in R: A(i) \implies B(i) $ & \{A, AB\}                             & \{A, AC\}                               \\
Some As are not Bs                                 & $\exists i \in R: A(i)  \centernot \implies B(i) $ & \{A, AB\}                             & \{AB, ABC\}                             \\
No As are Bs                                       & $\forall i \in R: A(i) \centernot \implies B(i) $  & \{A, AC\}                             & \{A, AB\} \\ \bottomrule
\end{tabular}
\caption{Logical meanings of example syllogistic sentences with examples of states that are consistent with the literal meanings and inconsistent with the literal meanings.}
\label{tab:sem}
\end{table}

\subsection{Interpreting the premises, as a listener}

We now introduce a probabilistic listener model that updates their beliefs about the states given the premises of the syllogism. 
The model's prior on states $P(s)$ is constructed by sampling a Bernoulli random variable for each of the $r \in R$ regions: $P(s) \propto \prod_{r \in R} P(r)$. 
The premises of a syllogism update this prior on states via the literal, truth conditional meaning of each of the premises $u$ (shown in Table \ref{tab:sem}):

\begin{equation}
L_0(s \mid u ) \propto P(s)\cdot \mathcal{L}(u, s) 
\label{eq:L0}
\end{equation}
\noindent where $\mathcal{L}(u, s)$ is the lexicon that encodes the literal meanings of the quantified utterances used in syllogisms. 
These meanings can resolve to deterministic outcomes (e.g., $\mathcal{L}(u, s) = 1$ if the utterance $u$ is true of $s$, and 0 otherwise). 
In practice, however, we assume a small amount of noise $\phi$ in the semantics such that logically true $\mathcal{L}(u, s)$ may be judged false with probability proportional to $1-\phi$; conversely, states which are literally incompatible the utterance with have a small ($\phi$) probability of being judged true (rather than a 0 probability).
As a result, the semantics in the model take on continuous values such that $\mathcal{L}(u, s) = 1-\phi$ or $\phi$, a technique introduced by \citeA{degen2020redundancy}.


The model of literal interpretation can be used to update the prior distribution on states (Venn diagrams) into a posterior distribution given the meaning of the premises of the syllogism. 
For the interpretation of a syllogism, $u$ is composed of two utterances (the two syllogistic premises): $u_1$ and $u_2$. 
Belief updating proceeds by assuming both utterances are true: 
\begin{equation}
L_0(s \mid u_1,  u_2) \propto P(s)\cdot \mathcal{L}(u_1, s) \cdot \mathcal{L}(u_2, s) 
\label{eq:L0premises}
\end{equation}


\begin{figure}[b]
\centering
\includegraphics[width = \textwidth]{diagrams_allAB_allBC.pdf}
\caption{Set of Venn diagrams (states) literally compatible with the premises of the logically valid syllogism: \emph{All As are Bs}, \emph{All Bs are Cs}. The conclusion relation \emph{All As are Cs} is true in every compatible state.}
\label{fig:AAvenns}
\end{figure}


Equation \ref{eq:L0premises} defines a probability distribution over states (Venn diagrams) given the premises of the syllogism. 
Logically valid syllogisms give rise to a distribution over states where a quantifier relationship between the conclusion terms of the syllogism is true in all possible states given the premises. 
For example, in the logically valid syllogism  \emph{All As are Bs}, \emph{All Bs are Cs} (the ``Barbara'' syllogism), the relationship \emph{All As are Cs} is true in every possible state in which the premises are true; in particular, the region \emph{ABC} (an object which has all three properties) is true in every possible state (Figure \ref{fig:AAvenns}).
On the other hand, the premises of logically invalid syllogisms tend to give rise to many possible states in which no particular syllogistic conclusion is true in every state; some conclusions are true in more states, however, than other conclusions.
For example, in the logically invalid \emph{All As are Bs}, \emph{All Cs are Bs}, the conclusion \emph{No As are Cs} is logically possible though is true in fewer states than \emph{Some As are Cs} (Figure \ref{fig:EIvenns}).
The fact that our model reasons about a distribution of states that are likely given the premises enables it to draw conclusions from sets of premises that have no logically necessary (i.e., valid) conclusions.


\begin{figure}[t]
\centering
\includegraphics[width = \textwidth]{diagrams_allAB_allCB.pdf}
\caption{Set of Venn diagrams (states) literally compatible with the premises of the logically invalid syllogism: \emph{All As are Bs}, \emph{All Cs are Bs}. No Aristotelean syllogistic conclusion is true in every compatible state.}
\label{fig:EIvenns}
\end{figure}




\subsection{Producing a conclusion, as a speaker}

The second component of a syllogistic reasoning model is a model of conclusion production. 
That is, given a set of beliefs about the likely state of affairs (Eq.~\ref{eq:L0premises}), a speaker decides what conclusion should be drawn.
We formalize three models of conclusion production.
The first is a literal speaker, who selects conclusions that are true given the speaker's beliefs about the state. 
The second is a pragmatic speaker, who selects a conclusion that would best convey information about an individual likely state to a naive listener (\emph{State communication}). 
Our third and most sophisticated model is of a pragmatic speaker who chooses utterances to convey information about her entire belief distribution over states (\emph{Belief alignment}). 


\subsubsection{Space of conclusions}

The syllogistic reasoning task presents participants with a set of options for the quantifier relationship between the two terms of the syllogisms that are not described explicitly in relation to each other in the premises (i.e., the terms \emph{A} and \emph{C} if the premises are of the form \emph{A -- B}, \emph{B -- C}.). 
Particular tasks differ in the set of options that are given to participants, but the maximal set includes the four quantifiers  (\emph{all}, \emph{some}, \emph{not all}, \emph{none}) crossed with the two orderings of the terms of the conclusion (i.e., \emph{A -- C} or \emph{C -- A}).
Thus, there are eight quantified conclusion options. 

A ninth unique option, however, is also available: the option that \emph{nothing follows} (sometimes described as \emph{no valid conclusion}). 
The \emph{nothing follows} conclusion is the logically correct answer for the logically invalid syllogisms and is produced to variable extents by participants in both logically valid and invalid syllogisms \cite{Khemlani2012}.
While the semantics of the quantifier conclusions are quite clear (Table \ref{tab:sem}), the proper treatment of the \emph{nothing follows} conclusion is not obvious. 
Extant theories treat this option indirectly, either as a byproduct or last resort of a reasoning process, when it is treated at all \cite{ragni2019does, riesterer2020modeling}.
We approach the question of the meaning of \emph{nothing follows} from a language production standpoint, where we formalize the statement that \emph{nothing follows} as being tantamount to not saying anything at all. 
Formally, we model the \emph{nothing follows} statement as a vacuous, or silent, utterance: an utterance which is true in every possible state of affairs.
Hence, the impact of using such an utterance is that it does not serve to update a listener's beliefs about the state. 


\subsubsection{Literal production of conclusions}

Given a belief updating model based on the premises of a syllogism $L_0$, 
we define a literal production model $S_0$ that uses the listener $L_0$'s posterior distribution on states to determine what conclusion is likely to be true given the premises. 
This model $S_0$ can be viewed as one that is sampling a state from the listener's posterior distribution on states and randomly selecting among the conclusions that are literally true of that state. 
For a tighter comparison to the other models, we include a soft-max temperature parameter $\alpha$ that interpolates between probability matching behavior and probability maximizing behavior. 

\begin{equation}
S_0(u_3 \mid u_1, u_2) \propto \exp[ \alpha \cdot \sum_s \mathcal{L}(u_3, s) \cdot L_0(s \mid u_1, u_2)] \label{eq:R0}
\end{equation}

Here, $\mathcal{L}(u_3, s)$ is assumed to be a deterministic semantic function (unlike that inside $L_0$ which is a function of noise parameter $\phi$) that conditions on the literal meaning of the utterance $u_3$.


\begin{figure}[b]
    \centering
        \includegraphics[width = 0.95\textwidth]{venn_literal_AA1_AI1_AA2_exp.pdf}
    \caption{A representation of the literal listener model's belief distribution over states. The diagram shows the marginal posterior means over regions. The opacity in each region denotes the posterior probability of that region (object type) being true of the premises. The probability of a particular state is proportional to the joint probability of each region being true.}
    \label{fig:lit_state_qud}
\end{figure}



\subsubsection{Informative production of conclusions: State communication}

We define a more sophisticated speaker model who selects conclusions in order to convey information to a naive listener (who has heard neither premises nor conclusion). 
Since the reasoner does not know the state (but only has beliefs about the state given the premises of the syllogism), we operationalize the informational utility of a conclusion by marginalizing over the speaker's beliefs about the state.
This speaker model first samples a state from the conditional distribution on states given the premises (as did the literal speaker in Eq.~\ref{eq:R0}).
Following standard practice in the Rational Speech Act modeling framework, the utility of the utterance for a particular state is the negative surprisal of the state given the utterance.
The speaker then chooses utterances soft-max optimally with respect to that utility function.

\begin{equation}
S_1(u_3 \mid u_1, u_2) \propto \exp{ [ \alpha \cdot  \sum_s  \ln L_0(s \mid u_3) \cdot L_0(s \mid u_1, u_2) ] } \label{eq:R1a}
\end{equation}

Note that the two $L_0$'s in Eq.~\ref{eq:R1a} are actually two different agents.
$L_0(s \mid u_1, u_2)$ is the reasoner who is acting as a listener updating their beliefs about the state given the premises.
$L_0(s \mid u_3)$ is a hypothetical listener that the reasoner, when acting as a speaker, is communicating with; this hypothetical listener updates their beliefs about the state from the conclusion ($u_3$).
The priors and likelihoods of these agents are equivalent and thus are represented equivalently. 


\subsubsection{Informative production of conclusions: Belief alignment}

Rather than the speaker sampling a state and then selecting an utterance to describe that state, the informational utility can be defined with respect to the reasoner's full belief distribution over states (Figure \ref{fig:lit_state_qud}; \citeNP{Goodman2013, scontras2018probabilistic}). 
The utility of a conclusion, then, is quantified by how well it would align the listener's beliefs with those of the reasoner. 
Formally, we use the information-theoretic measure of Kullback-Leibler (KL) divergence as the measure of alignment of the naive listener and reasoner's belief distributions. 
We denote the reasoner's belief distribution over the state as $S= L_0(s \mid  u_1,u_2)$. 

\begin{align}
  \label{eq:KL-divergence}
  \text{KL}({ S \mid \mid L_0}(u)) = \sum_{s}  S(s) \ \log \frac{ S(s)}{{L_{0}}(s \mid u)}
\end{align}

\noindent Then, the speaker model that selects conclusions in an informative manner is given by: 

\begin{equation}
S_1(u_3 \mid u_1,  u_2) \propto  \exp [ \alpha \cdot - \text{KL}({ L_0(s \mid  u_1,u_2) \mid \mid L_0}(s \mid u_3)) ]  \label{eq:R1b}
\end{equation}

\noindent where $L_0(s \mid u)$ is the literal listener model defined in Equation \ref{eq:L0}. 

We note that a critical ingredient to this Belief Alignment model of conclusion production is the noisy semantics we assume inside the literal listener $L_0$.
Without it, the speaker would be required to say \emph{nothing follows} for any logically invalid syllogism. 
A logically invalid syllogism is one in which no conclusion is true in every state in which the premises are true.
If a reasoner/speaker were to choose one of these conclusions, then the states that are incompatible with the conclusion would be imagined (by the hypothetical listener) to have 0 probability. 
But since the conclusion is not logically valid, the reasoner/speaker believes these states to have non-0 probability.
This combination leads Eq.~\ref{eq:KL-divergence} to be undefined, which results in the model always saying \emph{nothing follows} for logically invalid syllogisms.
By adding a small amount of noise into the semantics, we avoid this boundary case.



\section{Modeling the Ragni et al. (2019) data set}

We test our models using a dataset published by \citeA{ragni2019does}. 
The data set consists of the results of a web experiment in which participants ($n = 139$) provided conclusions to all 64 syllogisms. In the experiment, participants completed all syllogistic reasoning problems after a brief training phase. Participants responded by selecting one of the nine possible responses for the conclusion of the syllogism (4 quantifier choices x 2 term orders + 1 \emph{nothing follows}). 
We use this data set because it is large and comprehensive (i.e., has many responses for all 64 syllogisms). 




\subsection{Bayesian data analysis}

To assess how well our models can accommodate the syllogistic reasoning data from \citeA{ragni2019does}, we embed each model variant (literal speaker, state communication, belief alignment) within a Bayesian data analysis model to infer the credible values of the latent parameters of the models and generate predictions given those parameters. 

\subsubsection{Parameters and priors}

The Rational Speech Act modeling framework assumes speakers are approximately rational agents who produce utterances as actions whose utility is defined by conveying information to a listener. The degree of rationality of the speaker interpolates between probability matching behavior and utility maximizing behavior. 
For the speaker rationality parameter $\alpha$, we assume a uniform prior over the range of (0, 10), consistent with the prior literature on RSA models. 
We additionally assume a small amount of noise in the literal semantics of the utterances such that with a small probability $\phi$, the literal listener disregards the utterance heard and does not update its beliefs via that utterance; we put a uniform prior over the limited range of (0, 0.1) over this parameter since we assume the noise to be a small quantity. 
For simplicity, we parametrize the prior distribution over states $P(x)$ in the reasoner models in a strictly uninformative manner: all regions (object types) have equal probability of being true, which is fixed to 0.5.\footnote{
We experimented with more flexible parameterizations of the prior but found that these did not significantly improve model performance on this data set.}

Finally, we assume one global ``order bias'' parameter that modulates the pragmatic reasoner's prior preferences for conclusions with a specific ordering of the terms (A--C vs. C--A) based on the ordering of the terms in the premises. 
These preferences are sometimes referred to as ``figural effects'', tracing back to the ``figure'', or term orderings, of the premises \cite{Wetherick1990, rips1994}.
Our goal is not to provide an explanation for these effects, but rather posit an effect in line with the observations of \citeA{JL1984, JL1978, geurts2003reasoning} that participants prefer conclusions whose subject term matches the subject of one of the premises.
Formally, the conclusion prior is parameterized by unnormalized probability weights, where a uniform prior would have a weight of 1 for all conclusions. 
We introduce a global preference  parameter $\beta$ for conclusions whose order of the terms (A--C vs. C--A) matches the order of the terms in the premises.
This first-term preference works by checking whether either A or C appear in the subject position of one of the premises.
If only one appears in the subject (e.g., the syllogistic frame \emph{A--B/B--C}), then conclusions where that same term appears in the subject (e.g., \emph{A--C}) will have a prior preference determined by $\beta$.
If both or neither appear in the subject (e.g., \emph{A--B/C--B} or \emph{B--A/B--C}), then no preference is given in the conclusion prior, corresponding to an unnormalized probability weight of 1. 
We put a uniform prior between (1, 5) on this preference weight since we assume it to be a preference for producing conclusions that match the order of the terms of the syllogism. 


\begin{figure}[ht!]
\begin{center}
\begin{tabular}{cc}
\begin{tikzpicture}
%% RSA nodes and data
\node[obs](data){$d_{ij}$};

\node[det, above=of data](R1){$R_{1}(c \mid \vec{p}_j)$};
\node[det, above=of R1](L0){$L_{0}$};

\node[det, left=of L0, yshift=1cm](x){$x$};
\node[const, left=of x](theta){$\theta_k = 0.5$};

\node[det, right=of L0, yshift=1cm](lex){$\mathcal{L}$};
\node[latent, right=of lex](phi){$\phi$};

\node[det, right=of R1, yshift=1cm](c){$c$};
\node[latent, right=of c](b0){$\beta$};

\node[latent, right=of c, yshift=-1cm](alpha2){$\alpha$};

\gate {RSAgate} {(R1)(L0)(lex)(x)(c)} {} ; %

\plate{plate_region}{(theta)}{$r \in \text{region}$};
\plate{plate_data}{(data)}{$i \in \text{subject}$};

\plate{plate_syllogism}{
 (R1)(data)(plate_data)
}{$j \in \text{syllogism}$}


\edge{R1}{data};
\edge{L0}{R1};


\edge{theta}{x};

\edge{x}{L0};

\edge{lex}{L0};
\edge{phi}{lex};


\edge{c}{R1};
\edge{b0}{c};

\edge{alpha2}{R1};

\node[text width=1cm] at (0.15,5.8) {\emph{\small{RSA}}};

\node[draw, align=left, execute at begin node=\setlength{\baselineskip}{3ex}] at (6.5, 5) { 
\text{\underline{Data-analysis Priors} } \\ 
$ \phi  \sim \text{Uniform}(0, 0.1)$ \\
$ \beta  \sim \text{Uniform}(1, 5)$ \\
$ \alpha \sim \text{Uniform}(0, 10)$ 
};

\end{tikzpicture}

    \end{tabular}
  \end{center}
  \caption{\small Bayesian data analysis model for Rational Speech Act model variants. The conclusion choices for each $i$ participant on each $j$ syllogism $d_{ij}$ are generated via the pragmatic reasoner model $R_1$, which defines a conditional distribution on conclusions $c$ given premises $\vec{p}_j$. The reasoner model $R_1$ is a Rational Speech-Act (RSA) model that interprets premises according to a literal listener model $L_0$, which updates its beliefs according to the literal meaning $\mathcal{L}$ of the sentences it hears. Literal meanings are the standard, truth-functional meanings of the quantifiers (Table 1\label{tab:sem}) with a small amount of noise $\phi$. The prior distribution over states $P(x)$ is parameterized by independent Bernoulli weights $\theta_k$ for each of the seven regions $r$ of a Venn diagram, which we assume are all equal to 0.5. The reasoner $R_1$'s prior distribution over conclusions includes a preference $\beta$ for term-orderings (A--C vs. C--A) that match the ordering of the terms in the premises. Finally, the reasoner $R_1$ selects conclusions according to the decision rule of the model variant (Literal Speaker, State Communication, Belief Alignment), each which has a soft-max parameter $\alpha$.}
  \label{fig:bayesnet}
\end{figure}

\subsubsection{Implementation and inference}

We implemented the cognitive models and the Bayesian data analytic models in the probabilistic programming language WebPPL \cite{dippl}; model code and data analysis scripts can be found at \url{https://github.com/mhtess/syllogism-paper}. 
To learn about the credible values of the parameters, we ran 3 MCMC chains each for 15,000 iterations discarding the first 5,000 iterations for burn-in. 
We assessed convergence through visual comparison of the posterior distributions to ensure that the inferences that resulted from looking at different chains were not appreciably different.\footnote{
The full Bayesian data analytic model is unidentifiable owing to symmetries in the parameter space. Thus, a convergence statistic 
such as the Gelman-Rubin convergence statistic $R$ \cite{gelman1992inference} is not a good indicator for the reliability of the posterior predictions from these models. Accordingly, the $R$ values for these MCMC chains were relatively high, indicating high chain variability. Notably, the $R$ values for the posterior predictions were substantially lower than those for the parameters (most $R$ values for the predictions fell between 1.3 and 1.4 whereas the $R$ values for the parameters ranged between 1.75 and 2.5). The conclusions we draw thus are primarily about the model predictions and model comparison; less important (and less reliable) are the precise parameter estimates from these models.
}
We performed model comparison by computing Bayes Factors, which quantify the likelihood of the data under each model averaging over the model's prior distribution over parameters. By averaging over each model's prior on parameters, Bayes Factors implicitly penalize models with extra degrees of freedom \cite{lee2014bayesian}.
We estimated the marginal likelihoods of the data under each model using an Annealed Importance Sampling algorithm \cite{neal2001annealed}.

\subsubsection{Results}

Overall, we found that the Belief Alignment pragmatic speaker model predicted the empirical data the best and provided the most parsimonious explanation of the three models.
This model performed several orders of magnitude better at explaining the data, taking into the account model complexity (in comparison to the Literal Speaker model, $\log BF \approx 300$; in comparison to the State Communication pragmatic speaker $\log BF \approx 1200$).
It also achieved the highest correlation coefficient and lowest mean squared error of the RSA model variants (Table \ref{tab:altStats}).

\begin{figure}[t]
\centering
\includegraphics[width = \textwidth]{bda_rsa_scatters_0paramPrior.pdf}
\caption{Posterior predictive model fits for the literal speaker model and two versions of the pragmatic speaker model. Each dot represents one of nine possible conclusions for one of the 64 possible syllogisms. Error bars denote bootstrapped 95\% confidence intervals for the empirical data. Posterior model predictions did not exhibit significant variance; the horizontal error bars that are large enough to see represent the 95\% Bayesian credible intervals for the model predictions.}
\label{fig:scatters}
\end{figure}

The Belief Alignment Pragmatic Speaker model captures a number of interesting patterns in the human syllogistic reasoning behavior. 
By incorporating a notion of informativeness into the selection of a conclusion, the model is able to  show a preference among equally true conclusions. For example, in the \emph{All As are Bs / All Bs are Cs} syllogism, the conclusion \emph{Some As are Cs} is true in every possible world that \emph{All As are Cs} is true, because \emph{all} entails \emph{some}. Nonetheless, the Belief Alignment pragmatic speaker model and human participants strongly prefer the \emph{all} conclusion (Figure \ref{fig:bars}, top-left facet); the model arrives at this preference because \emph{all} more informatively conveys the belief distribution of the speaker in comparison to \emph{some}.

Syllogisms for which the premises provide little information about the conclusion also reveal interesting reasoning patterns in both participants and the models.
In the \emph{No As are Bs / No Bs are Cs} syllogism, participants tend to respond either \emph{nothing follows} (the deductively correct conclusion) or either \emph{No As are Cs} or \emph{No Cs are As} (Figure \ref{fig:bars}, second row, second column). 
The Literal Speaker model predicts what is likely to be true given the premises: \emph{Some As are not Cs} is the most likely state of affairs. 
The State Communication model tries to be more informative, but because it only reasons about a particular state (Venn diagram) at a time, the more informative conclusion is to say \emph{Some As are Cs}, at the same time down-weighting the \emph{nothing follows} conclusion.
The Belief Alignment model takes a different approach: By trying to convey its entire distribution of belief states, the more informative conclusion it draws is either to say \emph{No As are Cs} or \emph{nothing follows}, most closely matching the human data. 

The model is also able to adjust its conclusions based on the subtle shifts in the meaning of the premises. One interesting three-way comparison is the \emph{Some As are Bs / Some Bs are Cs} vs. \emph{Some As are Bs / Some Bs are not Cs} vs. \emph{Some As are not Bs / Some Bs are not Cs} syllogism  (Figure \ref{fig:bars}, bottom-right quadrant).
For \emph{Some / Some}, the most likely and useful conclusion is \emph{Some}.
By changing the second premise to \emph{Some Bs are not Cs} (\emph{Some / Some not} syllogism), the modal conclusion changes to \emph{Some not}.
Yet changing the first premise also to \emph{Some As are not Bs} (\emph{Some not / Some not} syllogism) does not change the modal conclusion, which is also \emph{Some not}. 
Both the Belief Alignment model and the Literal Speaker are able to capture these subtle changes, revealing that the literal meanings of the premises are shifting participants choice of conclusions. 

By providing a substantive, information-theoretic hypothesis about the communicative function of the \emph{nothing follows} conclusion, the Belief Alignment model exhibits the highest dynamic range among the RSA models in predictions about how and when people are likely to assert that an argument has no valid conclusion (Figure \ref{fig:scatternvc}). The model's predictions automatically accord with two intuitive boundary conditions: For syllogisms for which there is an informative and valid quantifier conclusion (e.g., \emph{All As are Bs / All Bs are Cs}), the Belief Alignment model strongly disprefers the \emph{nothing follows} response.
For other syllogisms that have no logically valid conclusion and do not strongly imply other conclusions, the Belief Alignment model puts substantial probability mass on the \emph{nothing follows} conclusion.
Between these two extremes, and across the full data set of arguments, the Belief Alignment model explains a modest amount of the quantitative variability in people's preferences for the \emph{nothing follows} conclusion ($r_{belief} = 0.64, r_{belief}^2 = 0.40, \text{MSE}_{belief} = 0.017$) and more than either the State Communication Speaker Model ($r_{state} = 0.31, r_{state}^2 = 0.10, \text{MSE}_{state} = 0.04$) or the Literal Speaker Model ($r_{literal} = 0.44, r_{literal}^2 = 0.19, \text{MSE}_{literal} = 0.025$).

\begin{figure}[t]
\centering
\includegraphics[width = 0.85\textwidth]{bda_rsa_scatters_0paramPrior_nvc.pdf}
\caption{Posterior predictive model fits for the \emph{nothing follows} conclusion (a subset of the data shown in Figure 7). The Belief Alignment model shows the greatest range of predictions, owing to the formalization of the \emph{nothing follows} conclusion as a semantically-vacuous utterance. Error bars denote bootstrapped 95\% confidence intervals for the empirical data. Posterior model predictions did not exhibit significant variance; the horizontal error bars that are large enough to see represent the 95\% Bayesian credible intervals for the model predictions.}
\label{fig:scatternvc}
\end{figure}


The Belief Alignment model's inferred parameter values provide more insight into how the model is able to make the predictions it does.
The MAP and 95\% credible interval for the figural effect (first-term preference) parameter was  $\beta = 2.01 (1.82, 2.14)$, showing that our model prefers conclusions whose terms match those of the premises by a factor of 2-to-1. 
The semantic noise parameter was inferred to be $\theta = 0.06 (0.02, 0.06)$, showing that even a little bit of noise in the semantics aids the model in making better predictions.
Finally, the speaker optimality parameter was  $\alpha = 6.88 (1.43, 7.15)$, covering a range of values consistent with what is typically observed in the RSA literature. 

\begin{figure}[t]
\centering
\includegraphics[width = \textwidth]{bda_rsa_bars_0params.pdf}
\caption{Model predictions and behavioral results for the 9 possible conclusions to 16 syllogisms. Syllogisms are of the form: \emph{\_\_ As are Bs. \_\_ Bs are Cs.} Error bars denote bootstrapped 95\% confidence intervals for the human data and 95\% Bayesian credible intervals for the model predictions.}
\label{fig:bars}
\end{figure}




\subsection{Comparison to other theories}

Our modeling approach is a Computational or Rational Level of analysis model \cite{marr1982vision, anderson1990adaptive} that seeks to describe the computations that underly human syllogistic reasoning without stipulating a precise implementation of such a reasoning process. 
Many other theories of syllogistic reasoning instead consider the process by which a human reasoner solves a syllogism.
The two most influential theories of human syllogistic reasoning are the Mental Models Theory \cite{johnsonlaird2006we, khemlani2013processes} and the Probability Heuristics Model \cite{Chater1999}, each of which share features with our logic, probability, and pragmatics approach. 


\begin{figure}[t]
\centering
\includegraphics[width = \textwidth]{alternative_model_scatters.pdf}
\caption{Performance of alternative theories. Each point represents an individual premise and conclusion pair. Group-level models use parameters that are fit to the group-level data. Individual-level models use parameters specific to individual participants. }
\label{fig:altModels}
\end{figure}

\subsubsection{Alternative models}

\paragraph{Mental Models}

The Mental Models Theory considers reasoning over a set of iconic models, in which individual entities are represented explicitly, not unlike our state space of Venn Diagrams. 
The reasoning process under this theory proceeds by first constructing a mental model, proposing a conclusion that can be drawn from such a model, and then engaging in a search for counterexamples to that conclusion; if a counterexample is found, either the mental model is revised and the search for a conclusion continues or the reasoner concludes \emph{nothing follows}. 

Here we compare to a recent implementation of this theory \cite<the mReasoner model>{khemlani2013processes}, which is also probabilistic in nature (e.g., the model determines the number of objects to represent in the model by sampling  from a Poisson distribution). 
The model has four free parameters (canonically denoted $\lambda$, $\epsilon$, $\sigma$, and $\omega$): 
$\lambda$ determines the number of entities represented in the model, $\epsilon$ determines the kind of mental model constructed (either a canonical or non-canonical representation of the entities consistent with the syllogism; as described by \citeNP{bucciarelli1999strategies}), $\sigma$ denotes the probability of engaging in a search for counterexamples, and $\omega$ modulates the conclusion selection process that results when engaging in a search for counterexamples.  

\paragraph{Probability Heuristics} 

The Probability Heuristics model of \citeA{Chater1999} seeks to account for human syllogistic reasoning as a process of determining probabilistically valid (``p-valid'') conclusions, which can be approximated through a number of heuristics. 
Two heuristics generate candidate quantifiers for the conclusion, which use a notion of informativity for which \citeA{Chater1999} define a rank ordering: \emph{all} $>$ \emph{some} $>$ \emph{none} $>$ \emph{some...not}.  
The ``min-heuristic'' takes the quantifier of the least informative premise to be a potential conclusion quantifier, and the ``p-entailment heuristic'' further allows for a quantifier that is entailed by the ``min-heuristic'' quantifier to also be a candidate quantifier for the conclusion (e.g., if \emph{all} is generated from the ``min-heuristic'', \emph{some} will be generated from the ``p-entailment heuristic''). 
A parameter (canonically referred to as $p_{ent}$) modulates which heuristic (min or p-entail) the reasoner uses to generate conclusions. 
A third heuristic (``attachment'') determines the ordering of the terms in the conclusion (A--C vs. C--A): this heuristic stipulates that the subject term of the conclusion should match the subject term of the least-informative premise, when allowable, similar in spirit to the $\beta$ ``order bias'' parameter used in the Bayesian data analysis of the Rational Speech Act models. 

Two more heuristics determine the confidence (or, probability) of each of the candidate conclusions. 
Foremost, the ``max-heuristic'' stipulates the the probability of a candidate conclusion is proportional to the informativity of the most informative premise.
The ``O-heuristic'' additionally downweights conclusions that use the ``some...not'' quantifier.
In the implementation of the Probability Heuristics model that we use, these heuristics are parameterized by 4 parameters, which govern the confidence placed in conclusions of each of the four quantifiers (all, some, some...not, and none).

\subsubsection{Quantitative comparison}

To compare the quantitative accuracy of these alternatives theories to our own, we analyzed the results of a recent study of these theories which focused on modeling the behavior of individual participants in the syllogistic reasoning task \cite{riesterer2020models}.
This study explored the predictive capabilities of the mReasoner and Probability Heuristics theories by treating the parameters of the models as individual-level parameters which could vary among participants (as well as the group-level version of the model, in which a single set of parameters governs the behavior of the population of participants).
\citeA{riesterer2020models} fit the parameters of these models to the same data set from \citeA{ragni2019does} that we modeled above with the Rational Speech Act models.\footnote{The predictions of these models were accessed via the companion repository to the paper: \url{https://github.com/nriesterer/cogsci-individualization}.}

Since \citeA{riesterer2020models} focus on predictive accuracy of the models for individual behavior, their implementation of the Probability Heuristics model treats the five model parameters as binary (0 or 1). This focus on predictive accuracy means that if a participant uses a heuristic $<50\%$ of the time, the best predictions that could be made would result by setting the corresponding parameter to 0. 
To examine the predictions of these theories in a manner more comparable to our analysis of the Rational Speech Act models, we treated the predictions of these alternative models for individual participants similar to how we treated the behavioral responses of individual participants. 
We constructed bootstrapped 95\% confidence intervals for the model predictions by resampling with replacement the predictions for each unique syllogism and conclusion. 



\begin{center}
\begin{table}[h]
\centering
\pgfplotstabletypeset[sci zerofill,
    col sep = comma,
    every head row/.style={before row = \toprule, after row = \midrule},
    every last row/.style={after row = \bottomrule},
    columns/model/.style={string type, column name={Model}, column type = l},
    columns/r/.style={string type, column name={$r$}, column type = l,  sci sep align, precision=2},
        columns/r2/.style={string type, column name={$r^2$}, column type = l,  sci sep align, precision=2},
   columns/mse/.style={fixed, column name={MSE}, column type = l, dec sep align, precision=3},	      
   columns/nParameters/.style={string type, column name={Parameters}, column type = l,  sci sep align, precision=3},
     ]{allModels_stats_0paramPriors.csv}\caption{Summary statistics for the RSA and alternative models predicting the data from Ragni et al. (2019). Alternative models were fitted to both group level data and individual level data. Individual-level variants use a new set of parameters for each individual participant in the data set ($n=139$).}\label{tab:altStats}
\end{table}
\end{center}

The mReasoner and Probability Heuristics models with parameters fit globally to the whole data set (group-level models) perform quite poorly overall (Figure \ref{fig:altModels}). 
The models largely make categorical predictions (1s or 0s) whereas the aggregated human data is graded.
With parameters fit to individual participants and predictions aggregated, the mReasoner and Probability Heuristics models perform quite a bit better and comparable to our Rational Speech Act models in terms of their correlation and mean squared errors, though the RSA models all perform better in terms of absolute mean squared error (Table 2).
In addition, The individual-level parameterization of the mReasoner and Probability Heuristics models use a different set of parameters for each of the 139 participants in the task, yielding a total number of parameters for the model that is 1-to-2 orders of magnitude larger than the the group-level versions of these models and our Rational Speech Act models (Figure \ref{fig:altModels}).\footnote{We discuss model complexity here in a coarse-grained manner. The PHM parameters in the \citeA{riesterer2020models} implementation are binary and thus do not add to model complexity in the same way as continuous parameters do. Furthermore, a hierarchical model for the individual subject-level parameters (i.e., partial pooling) could more thoroughly constrain the parameter space for the individual-level PHM model.
Still, we think that the approximate complexity of the models can be appreciated by their order-of-magnitude difference in the number of parameters.}
These results suggest that these alternative theories encoding process-level hypotheses may be well-suited to model an individual participant's reasoning behavior, but are insufficient to adequately describe the computational problem that human reasoners are aiming to achieve. 


\section{General Discussion}

Reasoning with syllogisms sits at the intriguing intersection of language and logic. 
Syllogisms are a logical system, but they are expressed using natural language quantifiers and invite natural language inferences. 
We introduced a probabilistic pragmatic perspective for syllogistic reasoning, where reasoning involves behaving both as a listener interpreting the premises of the syllogism and as a speaker deciding what conclusion to say.
We developed and tested a family of probabilistic models of syllogistic reasoning that incorporate a logical semantics with a generative model of situations and then formalize how considerations of pragmatic informativeness could affect the selection of conclusions.
We found that conclusions selected by participants were best explained by a model that chose its conclusion from a syllogism by reasoning about how well that conclusion would align a naive listener's beliefs with those of their own. 

Our proposal is that human syllogistic reasoning is part logical, part probabilistic, and part pragmatic. Our theory is a rational model that articulates the functional problem human syllogistic reasoning is aiming to solve, which is at its core a problem of communication. Our work builds on the goal articulated by \citeA{Chater1999} of building a rational model of syllogistic reasoning, and we do so without positing any heuristic solutions to the inference problems. Our model is also deeply connected to the Mental Models approach \cite{johnson1975models, johnson2015logic} in that the states of affairs that our model reasons over are mental models of the world. 
One subtle difference is that a Mental Models theory would typically reason about tokens or instances of categories whereas our model reasons about types. 

We contrasted two mechanisms by which pragmatic considerations can enter into the production of conclusions: one based on informatively describing a particular state (or, Venn diagram) and one based on conveying the reasoner's entire belief distribution over states (i.e., their distribution over possible Venn diagrams). 
The results, both quantitatively and qualitatively, provide overwhelming evidence that it is the latter notion of belief alignment which most closely tracks human reasoning patterns. 
This formalization of pragmatic considerations differs sharply from previous perspectives on pragmatics in syllogistic reasoning.
Most notably, \citeA{Roberts2001} explored the predictions of an approach most similar to our State Communication model: how a reasoner would choose Gricean-enriched quantifiers (e.g., \emph{some} which implies not all) for each of the possible states (Venn diagrams) implied by the premises (which could have also been pragmatically enriched). 
Consistent with the observations of \citeA{Roberts2001}, such a State Communication model fails to provide any quantitative improvement across the data set, while our Belief Alignment model does.



A major shortcoming of extant approaches to syllogistic reasoning is that they do not provide a clear and interpretable mechanism by the which the conclusion that \emph{nothing follows} can be rationally produced \cite{riesterer2020modeling}. 
We formalized \emph{nothing follows} as a semantically vacuous statement which does nothing to update the listener's beliefs. 
This kind of utterance is, in the semantic sense, always true, and thus the Literal Speaker produces this utterance with constant probability.\footnote{Technically, the literal speaker produces this vacuous utterance with probability inversely proportional to the number of conclusions that are semantically compatible with the premises, which will vary based on the logical (in)validity of the syllogism.}
The Belief Alignment speaker, by contrast, selects utterances in order to convey their full beliefs about the state to the listener. 
When their beliefs are not substantially changed by hearing the premises (i.e., when the premises are uninformative relative to the reasoner's prior beliefs), the speaker will have a positive incentive to say \emph{nothing follows} as that utterance would correspondingly not alter the hypothetical listener's beliefs, leading to strong alignment between speaker and listener beliefs. 
To our knowledge, this formalization is the first substantive hypothesis about how the \emph{nothing follows} conclusion can be rationally produced in the syllogistic reasoning task.

\subsection{Extending the framework}

Our approach to syllogistic reasoning is at its core a Bayesian approach, where the premises of a syllogism update a reasoner's prior beliefs about the relationship between three properties. 
In our modeling, we did not assume anything substantive about these prior beliefs, except that they were constant across all syllogisms. 
Prior beliefs can, however, form one basis for formalizing hypotheses about how the content of the syllogism (i.e., the content of the particular terms: A, B, C) affects reasoning, a topic known as ``belief bias'' that has been of interest to psychologists for decades \cite{Oakhill1989, Oakhill1993, Cherubini1998}.
For example, \citeA{tessler2015understanding} found that some content effects in syllogistic reasoning could be captured by empirically measuring the relevant prior beliefs and using them in a model similar to that presented here.
An alternative way in which a reasoner's beliefs could influence their reasoning is by a reasoner explicitly rejecting a premise (e.g., not accepting that \emph{All artists are bakers}).
A basic ``accept/reject premise'' mechanism is already included in our model via the semantic noise parameter $\phi$, the probability with which the reasoner does not update their beliefs using a premise of a syllogism, which we also assumed to be a constant term.\footnote{
The semantic noise parameter $\phi$ is a catch-all for many forms of uncertainty that the listener may hold, including but not limited to an explicit ``accept/reject premise'' inference, a listener's uncertainty about the precise meaning of a premise, or other forms of uncertainty on the part of the listener.
}
Even with a constant semantic noise parameter value (i.e., a constant noise in the likelihood), differential rejection rates for premises can arise via different prior beliefs about properties, due to Bayes' Rule (e.g., the degree to which \emph{All artists are bakers} is rejected depends on the prior beliefs about the properties of being an artist and being a baker).\footnote{
With the noise probability $\phi$, the literal listener does not update their beliefs and falls back on the prior, which could result in the states that are true of the premises receiving lower probability depending on whether they are \emph{a priori} plausible or implausible.
For example, given two correlated properties (e.g., \emph{is a politician} P, \emph{is corrupt} C) and constant noise in the semantics ($\phi$), when a listener reads an unlikely premise (e.g., \emph{No politician is corrupt}), the states with high posterior probability will be:
\begin{itemize}
    \item with probability $1 - \phi$ (the premise is true): $P, \neg C$; $\neg P, C$; $\neg P, \neg C$
    \item with probability $\phi$ (the premise is false): $P,C$ (the state which has high probability under the prior)
\end{itemize}
If instead, the second property C were ``selfless'' (rather than ``corrupt''), then the prior would provide more probability mass to the states other than $P,C$ (since the two properties are unlikely to co-occur).
(This example should generalize to other quantifiers due to their logical symmetries: $\text{some} = \neg \text{none}$, $\text{some are not} = \neg \text{all}$.)
The end result, then, is that states that are consistent with the premises would have differential posterior probabilities depending on if they are \emph{a priori} plausible vs. implausible, which is the same behavior in the model one would observe if the listener is explicitly rejecting vs. accepting premises via differential noise rates. 
}
Still, it is not clear that differential prior beliefs would yield all the flexibility that varying noise rates could generate.
Further work should investigate these prior beliefs and differential noise rates to formalize hypotheses about how world knowledge rationally affects reasoning, which could speak to outstanding debates about whether these effects should be considered a ``bias'' \cite<e.g.,>{Klauer2000, Garnham2005, Dube2011}. 

Our model uses the literal, natural language semantics of the premises of the syllogism to update the model's prior beliefs. 
To extend our approach to reasoning with other explicit quantifiers (e.g., \emph{most} and \emph{few}), one could either generalize the binary regions in the Venn diagram to have continuous values or articulate an ontology of states that makes use of token entities instead of types (see e.g., \citeNP{tessler2014some}, cf., \citeNP{geurts2003reasoning}).
Additionally, syllogisms that appear in natural discourse often make use of implicitly quantified, generic statements (e.g., \emph{Taxes on the wealthy will be modified}; see also \citeNP{khemlani2008syllogistic}); our approach could naturally be expanded to incorporate recent advances in the computational semantics of generics to account for syllogistic reasoning about generic premises \cite{tessler2019language}. 

Finally, this model could be used to consider the implications of pragmatic reasoning on the interpretation of the premises. 
Such a pragmatic interpretation model could be constructed as a standard Rational Speech Act pragmatic listener \cite{Frank2012a, goodman2016pragmatic, scontras2018probabilistic}; that is, treating the listener component of the model as a listener who reasons about why a speaker produced the premises that they did, where the speaker decides what utterance to produce in order to convey information about the state (or, belief distribution) to a naive literal listener.  
Indeed, we have explored several versions of this kind of pragmatic model in the course of this project (described briefly in the Appendix) but found that no model extension we tried provided a substantially better quantitative fit to Ragni et al. (2019) data than the model that interprets the premises literally, as we presented.\footnote{
Specifically, we formalized pragmatic listener models that interpret the premises from a rational speaker who wants to convey information about the full state as well as a speaker who wants to convey information about a number of different Questions Under Discussion \cite{Roberts2004QUD}, in which the speaker only cares about conveying information about the $A, C$ terms of the state; see also \citeNP{tessler2014some} for yet a different formulation of pragmatic interpretation of premises which achieves a comparable fit to that presented here.
}
We do not take this null result as evidence that pragmatics does not enter into the interpretation process of a syllogism.
We have only considered one particular experimental protocol and data set (from Ragni et al., 2019), where participants were presented with all 64 syllogisms; this experimental protocol potentially reveals to the participant the generative process of the syllogism (parametric recombination of quantifiers and term orders), which could induce a more logical analysis of the syllogisms.
As well, the participants were from Western, educated, industrialized, rich, democratic societies, a feature not shared with most other human beings \cite{henrich2010most}. 
Reasoning with syllogisms varies appreciably across individuals \cite{galotti1986individual, bacon2003individual} and substantially so in illiterate persons \cite{dias2005reasoning}.
In other contexts or populations, we may see that reasoning with syllogisms as a special case of reasoning about an argument or a riddle \cite<cf.>{luria1976cognitive, Ong1982}. 
Extending our modeling approach with a pragmatic interpretation component will be important for analyzing such behavior.\footnote{
It is additionally noteworthy that the pragmatic interpretation models we explored break the independence of the likelihood of utterances that is assumed by the literal interpretation model (Eq.~\ref{eq:L0}). We believe pragmatics provides a plausible mechanism by which the likelihood of otherwise independent utterances can become dependent (namely, via a rational speaker choosing to produce the two utterances).}

\subsection{Relationship to other frameworks for extending rationality}

Human reasoning with syllogisms has captured the attention of theoretical and empirical psychologists from the time of \citeA{Storring1908}, and many theoretical frameworks have been brought to bear on these capacities for the last half-century.
Our approach extends the structured, Bayesian perspective on cognition \cite{tenenbaum2011grow, goodman2014concepts} with principles of natural language, formalized in models of rational communication \cite{Frank2012a, Goodman2013, scontras2018probabilistic}. 
Many contemporary cognitive modeling frameworks are instead concerned with extending notions of rationality by incorporating computational constraints that the mind may operate within, and such approaches to these constraints might help explain the residual variability that our pragmatics models left unexplained.
We briefly discuss the relationship of our approach to three approaches to extending rationality: resource rationality, quantum models of cognition, and ecologically rational heuristics.

A recent powerful framework for the integration of processing constraints into rational models of cognition -- \emph{resource rational analysis} \cite{griffiths2015rational, lieder2020resource}  -- could be brought to bear directly on our probabilistic pragmatics perspective in a number of ways. 
In contrast to our Belief Alignment speaker model who aims to communicate their entire belief distribution over states, a resource rational speaker might instead draw a few samples from its posterior distribution on states and seek to convey that collection of states succinctly \cite<cf.,>{vul2014one}.
Furthermore, a resource rational pragmatic reasoner (e.g., behaving as a pragmatic listener) may not fully represent the knowledge state or goals of their interlocutor (the hypothetical speaker who produced the syllogism) due to computational constraints, leading to asymmetries in how the listener/reasoner responds to the speaker's argument~vs.~how the speaker might have intended \cite{hawkins2021division}; this kind of wrinkle introduced by resource rational considerations could have important implications for how the probabilistic  pragmatic perspective bears on the problem of syllogistic reasoning.

Quantum probability models of cognition are similar to Bayesian models but take as their starting point the axioms of quantum probability theory \cite{pothos2013can}. One key difference between quantum and classical theories of probability is the order dependence of conjunctive events under quantum theory. This lens in particular could be used for investigating deviations from Bayesian rational reasoning between logically identical syllogisms, whose premises are order reversed (e.g., \emph{Some As are Bs / All Bs are Cs} vs. \emph{All Bs are Cs / Some As are Bs}).
At the same time, such divergences could also result from a probabilistic pragmatics perspective, in which a speaker deciding to say two utterances in a particular order could lead to a non-independence of the likelihood of those utterances (see also Footnote 11). 

Heuristic accounts have also had success in explaining human syllogistic reasoning (e.g., the Probability Heuristics Model; \citeNP{Chater1999}); such a perspective could be extended with the contemporary adaptive toolbox approach, by incorporating the interaction between the cognitive agent and the environment \cite{hertwig2019taming}. 
Such an extension could be productive in describing the ecological setting in which human syllogistic reasoning can be best understood and how that might vary across culture. 
The heuristic perspective may, however, turn out to look a lot like a resource rational implementation of the Bayesian model (i.e., the line between an algorithmic implementation of rational inference and heuristics for inference may turn out to be superficial; see also e.g., \citeNP{lieder2018anchoring}). 


\subsection{Conclusion}


We articulate the verbal reasoning machinery that underlies the human capacity to reason with deductive arguments. 
This machinery is composed of three ingredients: logic, probability, and pragmatics. 
The combination of logic and probability enables us to reason about situations we have not experienced, but could encounter in the future.
The conclusions we draw are not just those that are plausible, however.
An agent needs to be posited at the other end of the line so that a conclusion makes sense; so that an argument may convince!


 \section{Acknowledgements}
 
We thank Erin Bennett, Sophie Bridgers, James Cargile, Justine Kao, Max Kleiman-Weiner, Robert Hawkins, and Long Ouyang for helpful conversations and insightful comments. 
This work was supported in part by NSF Graduate Research Fellowship DGE-114747 and NSF SBE Postdoctoral Research Fellowship Grant No.~1911790 to MHT, Army Research
Office MURI Grant No. W911NF-19-1-0057 to JBT, and a Sloan Research Fellowship, ONR grant N00014-13-1-0788, and DARPA grant FA8750-14-2-0009 to NDG.
The funders had no role in theory development, study design, modeling work, decision to publish or preparation of the manuscript.


\newpage

\bibliographystyle{apacite}
\bibliography{syllogism}

\newpage 

\appendix

\section{Appendix: Pragmatic interpretation of premises}

Understanding language often involves reasoning not only about the literal meaning of what was said but about why an interlocutor (the speaker) would bother to say what they said, which can lead to enriched interpretations beyond the literal meanings. 
We can formalize this reasoning through a series of Bayesian models that recursively reason about one another, as developed in the Rational Speech Act (RSA) framework \cite{Frank2012a, goodman2016pragmatic, scontras2018probabilistic}.
Concretely, we can describe a pragmatic listener $L_1$ who reasons about a speaker $S_1$ who produces utterances in order to convey information to the literal listener (Equation \ref{eq:L0premises}):

\begin{align}
L_1(s \mid u_1,  u_2)& \propto  P(s)\cdot S_1(u_1, u_2 \mid s)  \label{eq:L1} \\ 
S_1(u_1, u_2 \mid s) &\propto  \exp [ \alpha_1 \cdot \ln L_0(s \mid u_1,  u_2)]  \label{eq:S1}
\end{align}

The definition of the pragmatic listener (Equation \ref{eq:L1}) mirrors that of the literal listener (Equation \ref{eq:L0}) except in that the likelihood is defined as the probability that a speaker $S_1$ would produce utterances (premises) $u_1$ and $u_2$ given a state $s$.
Following standard practice in RSA modeling, the speaker is a soft-max rational agent (with degree of rationality $\alpha_1$) who produces utterances in order to convey information about the state $s$ to the literal listener (formalized by the negative surprisal of state given the utterances: $\ln L_0(s \mid u_1,  u_2)$).

\paragraph{Alternative utterances}
The proportionality in Equation \ref{eq:S1} implies a normalization over a set of alternative utterances (i.e., the set of sentences that the speaker could have produced); in our case, the set of alternative utterances amounts to a set of alternative syllogistic premises that a speaker could have produced. 
There are at least three sets of alternative premises that are \emph{a priori} plausible and we tested models using each of: (1) alternative premises that have the same ordering of terms (i.e., syllogisms of the same form, or ``figure''), but whose quantified relation between the terms could be different (e.g., if the speaker says \emph{All As are Bs / All Bs are Cs}, the alternative set is the set of syllogisms of the form \emph{$Q_1$ As are Bs} / \emph{$Q_2$ Bs are Cs} where $Q_1, Q_2 \in \{all, some, \emph{not all}, none\}$); (2) alternative premises that differ either in the quantified relation between terms or the ordering of the terms, but not both (i.e., utterances with an edit-distance of 1); (3) the maximal set of alternative premises in which the quantifier and the ordering of terms could be different.

\paragraph{Question Under Discussion (QUD)}
When deciding what to say, speakers may choose their utterances to convey information about a particular topic of relevance and listeners may assume that speakers had a particular topic of relevance in mind when interpreting the utterances heard. 
We formalize this topic as a Question Under Discussion (or, QUD), which acts as a function that projects the state space onto a lower-dimensional representation of the state space \cite{Roberts2004QUD, kao2014nonliteral}.
Though QUDs may not be the explicit question asked of a person \cite<e.g.,>{hawkins2015you}, in the context of a syllogistic reasoning problem, the most natural QUD is the explicit question asked of participants: What relationship between As and Cs follows from the premises? \cite{tessler2014some}.
In particular, we considered the QUD when interpreting the premises of a syllogism (\emph{As are Bs}, \emph{Bs are Cs}) to be about the A--C relationship. 
In our model, this QUD function acts by transforming a three circle Venn diagram (A, B, C) into a two circle diagram (A, C): $Q(\{A,B,C\}) = \{A,C\}$.


The pragmatic listener who interprets the premises relative to this QUD is given by a set of equations:

\begin{align}
L_1(s \mid u_1,  u_2, Q)& \propto  P(s)\cdot S_1(u_1, u_2 \mid s, Q)  \label{eq:L1q} \\ 
S_1(u_1, u_2 \mid s, Q) &\propto  \exp [ \alpha_1 \cdot \ln L_0(Q(s) \mid u_1,  u_2, Q)]  \label{eq:S1q} \\
L_0(Q(s) \mid u_1,  u_2) & = \sum_{s' \in \mathcal{S}}  \delta_{Q(s)=Q(s')} \cdot L_0(s'\mid u_1,  u_2) \label{eq:L0q}
\end{align}

In our explorations of the QUD model with various sets of alternative utterances, we did not find any substantial improvement in quantitative fit to the Ragni et al. (2019) data set. 

\end{document}
